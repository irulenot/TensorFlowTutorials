{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[1.0, 2.0, 3.0]], [[7.0, 8.0, 9.0]]]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3. # a rank 0 tensor; a scalar with shape [],\n",
    "[1., 2., 3.] # a rank 1 tensor; a vector with shape [3]\n",
    "[[1., 2., 3.], [4., 5., 6.]] # a rank 2 tensor; a matrix with shape [2, 3]\n",
    "[[[1., 2., 3.]], [[7., 8., 9.]]] # a rank 3 tensor with shape [2, 1, 3]\n",
    "\n",
    "#Tensors are basically matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might think of TensorFlow Core programs as consisting of two discrete sections:\n",
    "\n",
    "1) Building the computational graph (a tf.Graph).\n",
    "\n",
    "2) Running the computational graph (using a tf.Session).\n",
    "\n",
    "\n",
    "\n",
    "A Graph contains a set of tf.Operation objects, which represent units of computation; and tf.Tensor objects, which represent the units of data that flow between operations.\n",
    "\n",
    "A Session object encapsulates the environment in which Operation objects are executed, and Tensor objects are evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A computational graph is a series of TensorFlow operations arranged into a graph. The graph is composed of two types of objects.\n",
    "\n",
    "Operations (or \"ops\"): The nodes of the graph. Operations describe calculations that consume and produce tensors.\n",
    "Tensors: The edges in the graph. These represent the values that will flow through the graph. Most TensorFlow functions return tf.Tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const:0\", shape=(), dtype=float32)\n",
      "Tensor(\"Const_1:0\", shape=(), dtype=float32)\n",
      "Tensor(\"add:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# building a computational graph, we define graph \n",
    "# with tensors a and b, then use and operation of +\n",
    "\n",
    "a = tf.constant(3.0, dtype=tf.float32)\n",
    "b = tf.constant(4.0) # also tf.float32 implicitly\n",
    "total = a + b\n",
    "print(a)\n",
    "print(b)\n",
    "print(total)\n",
    "\n",
    "# total is an operation of add and produces a tensor add_1:0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.summary.FileWriter('.')\n",
    "writer.add_graph(tf.get_default_graph())\n",
    "\n",
    "#This will produce an event file in the current directory with a \n",
    "# name in the following format:\n",
    "# events.out.tfevents.{timestamp}.{hostname}\n",
    "\n",
    "# Now, in a new terminal, launch TensorBoard with the following \n",
    "# shell command: tensorboard --logdir .\n",
    "\n",
    "# localhost:6006 to open in browser\n",
    "# SUPER COOL!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate tensors, instantiate a tf.Session object, informally known as a session. A session encapsulates the state of the TensorFlow runtime, and runs TensorFlow operations. If a tf.Graph is like a .py file, a tf.Session is like the python executable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.0\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "print(sess.run(total))\n",
    "\n",
    "# When you request the output of a node with Session.run \n",
    "# TensorFlow backtracks through the graph and runs \n",
    "# all the nodes that provide input to the requested output node. \n",
    "# So this prints the expected value of 7.0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ab': (3.0, 4.0), 'total': 7.0}\n"
     ]
    }
   ],
   "source": [
    "print(sess.run({'ab':(a, b), 'total':total}))\n",
    "\n",
    "# can edit what sess run outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.788303   0.30530667 0.7705107 ]\n",
      "[0.7839005  0.76697135 0.84752464]\n",
      "(array([1.3130958, 1.7870046, 1.1058717], dtype=float32), array([2.3130958, 2.7870045, 2.1058717], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "vec = tf.random_uniform(shape=(3,))\n",
    "out1 = vec + 1\n",
    "out2 = vec + 2\n",
    "print(sess.run(vec))\n",
    "print(sess.run(vec))\n",
    "print(sess.run((out1, out2)))\n",
    "\n",
    "# The result shows a different random value on each call to run, \n",
    "# but a consistent value during a single run \n",
    "# (out1 and out2 receive the same random input):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.5\n",
      "[3. 7.]\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "z = x + y\n",
    "\n",
    "# A placeholder is a promise to provide a value later, \n",
    "# like a function argument.\n",
    "\n",
    "print(sess.run(z, feed_dict={x: 3, y: 4.5}))\n",
    "print(sess.run(z, feed_dict={x: [1, 3], y: [2, 4]}))\n",
    "\n",
    "# We can evaluate this graph with multiple inputs by using\n",
    "# the feed_dict argument of the run method to feed concrete \n",
    "# values to the placeholders:\n",
    "\n",
    "\n",
    "# A way of feeding in multiple things into a session at a time\n",
    "# or seperately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.5\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(5, dtype=tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "z = x + y\n",
    "print(sess.run(z, feed_dict={x: 20, y: 4.5}))\n",
    "\n",
    "# Also note that the feed_dict argument can be used to overwrite \n",
    "# any tensor in the graph. The only difference between placeholders \n",
    "# and other tf.Tensors is that placeholders throw an error \n",
    "# if no value is fed to them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Placeholders work for simple experiments, but Datasets are the preferred method of streaming data into a model.\n",
    "\n",
    "class Dataset: Represents a potentially large set of elements.\n",
    "class Iterator: Represents the state of iterating through a Dataset.\n",
    "method make_one_shot_iterator: Creates an Iterator for enumerating the\n",
    "    elements of this dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = [\n",
    "    [0, 1,],\n",
    "    [2, 3,],\n",
    "    [4, 5,],\n",
    "    [6, 7,],\n",
    "]\n",
    "# Ttf.Tensor from a Dataset to a tf.data.Iterator\n",
    "slices = tf.data.Dataset.from_tensor_slices(my_data)\n",
    "# then call the Iterator's get_next method\n",
    "next_item = slices.make_one_shot_iterator().get_next()\n",
    "\n",
    "# calling next_item, or slices.make_one_shot_iterator().get_next(),\n",
    "# makes the iterator traverse my_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "[2 3]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "print(sess.run(next_item))\n",
    "print(sess.run(next_item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "[2 3]\n",
      "[4 5]\n",
      "[6 7]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "while True:\n",
    "  try:\n",
    "    print(sess.run(next_item))\n",
    "  except tf.errors.OutOfRangeError:\n",
    "    break\n",
    "# To iterate til end, or else throws error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.2849698  0.39986426 0.8363965 ]\n",
      "[-0.22005548  0.7852679  -1.3876309 ]\n",
      "[ 0.9066339 -0.7323097 -0.1944773]\n",
      "[-0.0963688  -0.2940833  -0.90976125]\n",
      "[ 0.34254307 -0.5926556   0.83048093]\n",
      "[-1.2736316   1.5304605  -0.07646449]\n",
      "[-1.2000003  -1.5103297  -0.77437305]\n",
      "[ 1.652955    0.84029835 -0.85513127]\n",
      "[-0.33895183 -0.7121658   0.17647195]\n",
      "[-0.75605726 -0.49841708  1.1098886 ]\n"
     ]
    }
   ],
   "source": [
    "# If the Dataset depends on stateful operations you may need to initialize \n",
    "# the iterator before using it, as shown below:\n",
    "\n",
    "r = tf.random_normal([10,3])\n",
    "dataset = tf.data.Dataset.from_tensor_slices(r)\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "next_row = iterator.get_next()\n",
    "\n",
    "sess.run(iterator.initializer) \n",
    "#iterator.initializer creates new iterator so can start from the top\n",
    "while True:\n",
    "  try:\n",
    "    print(sess.run(next_row))\n",
    "  except tf.errors.OutOfRangeError:\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code creates a Dense layer that takes a batch of input vectors, and produces a single output value for each. To apply a layer to an input, call the layer as if it were a function. For example:\n",
    "\n",
    "class Dense: This layer implements the operation: outputs =\n",
    "    activation(inputs * kernel + bias)\n",
    "class Layer: Base layer class. This is the class from which all layers \n",
    "    inherit, implementing common infrastructure functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.1201149]\n",
      " [-1.3120627]]\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 3]) #example parameters\n",
    "linear_model = tf.layers.Dense(units=1) #Layer\n",
    "y = linear_model(x) #Output is layer acting on examples\n",
    "\n",
    "# The layer contains variables that must be \n",
    "# initialized before they can be used. Because they say so\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "print(sess.run(y, {x: [[1, 2, 3],[4, 5, 6]]})) # applying model on 2 inputs\n",
    "\n",
    "# tf.layers.dense is a shortcut but bad for debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class feature_columns: Think of feature columns as the intermediaries between raw data and Estimators.\n",
    "function Input column: only accepts dense columns as inputs, Returns a dense Tensor as input layer based on given\n",
    "    feature_columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making feature columns\n",
    "\n",
    "# The easiest way to experiment with feature columns is using \n",
    "# the tf.feature_column.input_layer function. \n",
    "# This function only accepts dense columns as inputs, \n",
    "# so to view the result of a categorical column you must wrap it \n",
    "# in an tf.feature_column.indicator_column. For example:\n",
    "\n",
    "features = {\n",
    "    'sales' : [[5], [10], [8], [9]],\n",
    "    'department': ['sports', 'sports', 'gardening', 'gardening']\n",
    "}\n",
    "\n",
    "#Defining columns\n",
    "department_column = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "        'department', ['sports', 'gardening'])\n",
    "department_column = tf.feature_column.indicator_column(department_column)\n",
    "\n",
    "columns = [\n",
    "    tf.feature_column.numeric_column('sales'),\n",
    "    department_column\n",
    "]\n",
    "\n",
    "inputs = tf.feature_column.input_layer(features, columns)\n",
    "#Running the inputs tensor will parse the features into a batch of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.  5.]\n",
      " [ 1.  0. 10.]\n",
      " [ 0.  1.  8.]\n",
      " [ 0.  1.  9.]]\n"
     ]
    }
   ],
   "source": [
    "# Feature columns can have internal state, like layers, so they often need to be initialized. \n",
    "# Categorical columns use lookup tables internally and these require a separate initialization op, \n",
    "# tf.tables_initializer.\n",
    "\n",
    "var_init = tf.global_variables_initializer()\n",
    "table_init = tf.tables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run((var_init, table_init))\n",
    "\n",
    "# Once the internal state has been initialized you can run inputs like any other tf.Tensor:\n",
    "print(sess.run(inputs))\n",
    "\n",
    "# ... hmm to be filled in, unsure how these categories and values are relevant as of yet. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you're familiar with the basics of core TensorFlow, let's train a small regression model manually. HAH!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.1777302]\n",
      " [-2.3554604]\n",
      " [-3.5331907]\n",
      " [-4.710921 ]]\n",
      "2.1255612\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([[1], [2], [3], [4]], dtype=tf.float32)\n",
    "y_true = tf.constant([[0], [-1], [-2], [-3]], dtype=tf.float32)\n",
    "\n",
    "linear_model = tf.layers.Dense(units=1)\n",
    "y_pred = linear_model(x)  # initalized with 1 layer with 1 input (unit)\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "print(sess.run(y_pred)) # since model not trained output sucks\n",
    "\n",
    "loss = tf.losses.mean_squared_error(labels=y_true, predictions=y_pred)\n",
    "print(sess.run(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To optimize a model, you first need to define the loss. We'll use the mean square error, a standard loss for regression problems.\n",
    "\n",
    "While you could do this manually with lower level math operations, the tf.losses module provides a set of common loss functions. You can use it to calculate the mean square error as follows\n",
    "\n",
    "TensorFlow provides optimizers implementing standard optimization algorithms. These are implemented as sub-classes of tf.train.Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1255612\n",
      "1.5104313\n",
      "1.0833931\n",
      "0.7868695\n",
      "0.58090764\n",
      "0.43778658\n",
      "0.33827022\n",
      "0.26901168\n",
      "0.22074969\n",
      "0.18705788\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(0.01) #input is step size\n",
    "train = optimizer.minimize(loss) #can use gradient descent after implement loss function in tf\n",
    "\n",
    "for i in range(10):\n",
    "  _, loss_value = sess.run((train, loss)) #train doesn't have output so use loss to see learning rate\n",
    "  print(loss_value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

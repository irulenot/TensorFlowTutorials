{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eager Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow's eager execution evaluates operations immediately, without an extra graph-building step. Operations return concrete values instead of constructing a computational graph to run later. This makes it easy to get started with TensorFlow, debug models, reduce boilerplate code, and is ~~fun~~ amazing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup and basic usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello, [[4.]]\n"
     ]
    }
   ],
   "source": [
    "tf.executing_eagerly()\n",
    "\n",
    "x = [[2.]]\n",
    "m = tf.matmul(x, x) \n",
    "print(\"hello, {}\".format(m))\n",
    "# tf.Tensor objects reference concrete values instead of \n",
    "# symbolic handles to nodes in a computational graph.           COOL\n",
    "# Since there isn't a computational graph to build and run later \n",
    "# in a session, it's easy to inspect results using print() or a debugger.\n",
    "\n",
    "# TensorFlow math operations convert Python objects \n",
    "# and NumPy arrays to tf.Tensor objects. The tf.Tensor.numpy    THANK GOD\n",
    "# method returns the object's value as a NumPy ndarray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1 2]\n",
      " [3 4]], shape=(2, 2), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[2 3]\n",
      " [4 5]], shape=(2, 2), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[ 2  6]\n",
      " [12 20]], shape=(2, 2), dtype=int32)\n",
      "-------\n",
      "[[ 2  6]\n",
      " [12 20]]\n",
      "[[1 2]\n",
      " [3 4]]\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([[1, 2],\n",
    "                 [3, 4]])\n",
    "print(a)\n",
    "\n",
    "# Broadcasting support\n",
    "b = tf.add(a, 1) #a+1\n",
    "print(b)\n",
    "\n",
    "# Operator overloading is supported\n",
    "print(a * b)     #a.*b    NEAT!!\n",
    "print(\"-------\")\n",
    "\n",
    "# Use NumPy values\n",
    "import numpy as np\n",
    "c = np.multiply(a, b)\n",
    "print(c)\n",
    "\n",
    "# Obtain numpy value from a tensor:\n",
    "print(a.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eager training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "[<tf.Tensor: id=31, shape=(1, 1), dtype=float32, numpy=array([[2.]], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.contrib.eager as tfe\n",
    "\n",
    "w = tfe.Variable([[1.0]])\n",
    "with tfe.GradientTape() as tape:\n",
    "  loss = w * w\n",
    "\n",
    "grad = tape.gradient(loss, [w])\n",
    "print(grad) # getting the hang of it, you can print out details of any\n",
    "            # tf object\n",
    "    \n",
    "# During eager execution, use tfe.GradientTape to trace \n",
    "# operations for computing gradients later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 68.494\n",
      "Loss at step 000: 65.857\n",
      "Loss at step 020: 30.238\n",
      "Loss at step 040: 14.185\n",
      "Loss at step 060: 6.946\n",
      "Loss at step 080: 3.681\n",
      "Loss at step 100: 2.206\n",
      "Loss at step 120: 1.540\n",
      "Loss at step 140: 1.240\n",
      "Loss at step 160: 1.104\n",
      "Loss at step 180: 1.042\n",
      "Final loss: 1.015\n",
      "W = 3.043541193008423, B = 2.1179614067077637\n"
     ]
    }
   ],
   "source": [
    "# A toy dataset of points around 3 * x + 2\n",
    "NUM_EXAMPLES = 1000\n",
    "training_inputs = tf.random_normal([NUM_EXAMPLES]) #generates 1000 #s 0-1\n",
    "noise = tf.random_normal([NUM_EXAMPLES])\n",
    "training_outputs = training_inputs * 3 + 2 + noise\n",
    "\n",
    "def prediction(input, weight, bias):\n",
    "  return input * weight + bias\n",
    "\n",
    "# A loss function using mean-squared error\n",
    "def loss(weights, biases):\n",
    "  error = prediction(training_inputs, weights, biases) - training_outputs\n",
    "  return tf.reduce_mean(tf.square(error)) \n",
    "    #handles matrices, very cool\n",
    "\n",
    "# Return the derivative of loss with respect to weight and bias\n",
    "def grad(weights, biases):\n",
    "  with tfe.GradientTape() as tape:\n",
    "    loss_value = loss(weights, biases) \n",
    "  return tape.gradient(loss_value, [weights, biases]) \n",
    "    # tape.gradient(squared_error, [theta, biases])\n",
    "    # returns derivatives of loss for each theta! neat. \n",
    "    # Note to self: seperate biases from weights\n",
    "\n",
    "train_steps = 200\n",
    "learning_rate = 0.01\n",
    "# Start with arbitrary values for W and B on the same batch of data\n",
    "W = tfe.Variable(5.) #theta\n",
    "B = tfe.Variable(10.) #theta(1) in octave\n",
    "#standard variable, in this case a decimal\n",
    "\n",
    "print(\"Initial loss: {:.3f}\".format(loss(W, B)))\n",
    "\n",
    "for i in range(train_steps):\n",
    "  dW, dB = grad(W, B)\n",
    "  W.assign_sub(dW * learning_rate) # W -=('gradient' * alpha)\n",
    "  B.assign_sub(dB * learning_rate)\n",
    "    #basically gradient descent for a simple linear classifier\n",
    "  if i % 20 == 0:\n",
    "    print(\"Loss at step {:03d}: {:.3f}\".format(i, loss(W, B)))\n",
    "\n",
    "print(\"Final loss: {:.3f}\".format(loss(W, B)))\n",
    "print(\"W = {}, B = {}\".format(W.numpy(), B.numpy()))\n",
    "\n",
    "# basically in this example: \n",
    "# tfe.GradientTape() as tape, \n",
    "# can do, tape.gradient(loss_value, [weights, biases]\n",
    "# which takes: cost_fcn, [theta(2:end), theta(1)]\n",
    "# returns: vector of derivative (-/+ int)\n",
    "\n",
    "# *Can also be used to do:\n",
    "# backpropagation,\n",
    "# derivatives in respect to things, \n",
    "# partial derivatives,\n",
    "# *And can be:\n",
    "# overloaded,\n",
    "\n",
    "# Beautiful. (& complicated!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and train models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eager execution encourages the use of the Keras-style layer classes in the \"tf.keras.layers\" module. Additionally, the \"tf.train.Optimizer\" classes provide sophisticated techniques to calculate parameter updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Dense(10, input_shape=(784,)),  # must declare input shape\n",
    "  tf.keras.layers.Dense(10)\n",
    "]) \n",
    "#tf includes Keras in it! Can derive own class from Keras as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Andd of course they updated the tutorial while I was in the middle of doing it. That confused me for a good 20 min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a model\n",
    "While you can use any Python object to represent a layer, TensorFlow has tf.keras.layers.Layer as a convenient base class. Inherit from it to implement your own layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Dense(10, input_shape=(784,)),  # must declare input shape\n",
    "  tf.keras.layers.Dense(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 784)\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor representing a blank image\n",
    "batch = tf.zeros([1, 1, 784])\n",
    "print(batch.shape)  # => (1, 1, 784)\n",
    "\n",
    "result = model(batch)\n",
    "# => tf.Tensor([[[ 0.  0., ..., 0.]]], shape=(1, 1, 10), dtype=float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset  # download dataset.py file\n",
    "dataset_train = dataset.train('./datasets').shuffle(60000).repeat(4).batch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a model, define a loss function to optimize and then calculate gradients. Use an optimizer to update the variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(model, x, y):\n",
    "  prediction = model(x)\n",
    "  return tf.losses.sparse_softmax_cross_entropy(labels=y, logits=prediction)\n",
    "    # loss function, tf.losses is lib with loss functions\n",
    "    # sparse_softmax_cross_entropy is evidently one of their functions\n",
    "\n",
    "def grad(model, inputs, targets):\n",
    "  with tf.GradientTape() as tape:\n",
    "    loss_value = loss(model, inputs, targets)\n",
    "  return tape.gradient(loss_value, model.variables)\n",
    "    # keras object has .variables, which nicely passes into tape.gradient\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "# Gradient optimizer is apperantly an object we use\n",
    "\n",
    "x, y = iter(dataset_train).next()\n",
    "print(\"Initial loss: {:.3f}\".format(loss(model, x, y)))\n",
    "\n",
    "# Training loop\n",
    "for (i, (x, y)) in enumerate(dataset_train):\n",
    "  # Calculate derivatives of the input function with respect to its parameters.\n",
    "  grads = grad(model, x, y)\n",
    "  # Apply the gradieant to the model\n",
    "  optimizer.apply_gradients(zip(grads, model.variables),\n",
    "                            global_step=tf.train.get_or_create_global_step())\n",
    "    # kinda gross, but the optimizer object gets the above inputs (model info)\n",
    "    # and a global step tensor to do gradient descent\n",
    "    # still better than implementing backpropogation myself! Hah (even if it might be fun...)\n",
    "    \n",
    "    \n",
    "  if i % 200 == 0:\n",
    "    print(\"Loss at step {:04d}: {:.3f}\".format(i, loss(model, x, y)))\n",
    "\n",
    "print(\"Final loss: {:.3f}\".format(loss(model, x, y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables and optimizers\n",
    "tfe.Variable objects store mutable tf.Tensor values accessed during training to make automatic differentiation easier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 68.354\n",
      "Loss at step 000: 65.693\n",
      "Loss at step 020: 29.883\n",
      "Loss at step 040: 13.890\n",
      "Loss at step 060: 6.747\n",
      "Loss at step 080: 3.557\n",
      "Loss at step 100: 2.133\n",
      "Loss at step 120: 1.497\n",
      "Loss at step 140: 1.212\n",
      "Loss at step 160: 1.086\n",
      "Loss at step 180: 1.029\n",
      "Loss at step 200: 1.004\n",
      "Loss at step 220: 0.992\n",
      "Loss at step 240: 0.987\n",
      "Loss at step 260: 0.985\n",
      "Loss at step 280: 0.984\n",
      "Final loss: 0.984\n",
      "W = 2.9868690967559814, B = 2.052278995513916\n"
     ]
    }
   ],
   "source": [
    "class Model(tf.keras.Model):\n",
    "  def __init__(self):\n",
    "    super(Model, self).__init__()\n",
    "    self.W = tfe.Variable(5., name='weight') \n",
    "    self.B = tfe.Variable(10., name='bias')\n",
    "  def predict(self, inputs):\n",
    "    return inputs * self.W + self.B\n",
    "    # can initalize custom keras models and configure own\n",
    "    # thetas, bias, and prediction\n",
    "\n",
    "\n",
    "# A toy dataset of points around 3 * x + 2\n",
    "NUM_EXAMPLES = 2000\n",
    "training_inputs = tf.random_normal([NUM_EXAMPLES])\n",
    "noise = tf.random_normal([NUM_EXAMPLES])\n",
    "training_outputs = training_inputs * 3 + 2 + noise\n",
    "\n",
    "# The loss function to be optimized\n",
    "def loss(model, inputs, targets):\n",
    "  error = model.predict(inputs) - targets\n",
    "  return tf.reduce_mean(tf.square(error))\n",
    "\n",
    "def grad(model, inputs, targets):\n",
    "  with tfe.GradientTape() as tape:\n",
    "    loss_value = loss(model, inputs, targets)\n",
    "  return tape.gradient(loss_value, [model.W, model.B])\n",
    "    # utilize GradientTape in gradient descent\n",
    "\n",
    "# Define:\n",
    "# 1. A model.\n",
    "# 2. Derivatives of a loss function with respect to model parameters.\n",
    "# 3. A strategy for updating the variables based on the derivatives.\n",
    "model = Model()\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "\n",
    "print(\"Initial loss: {:.3f}\".format(loss(model, training_inputs, training_outputs)))\n",
    "\n",
    "# Training loop\n",
    "for i in range(300):\n",
    "  grads = grad(model, training_inputs, training_outputs)\n",
    "  optimizer.apply_gradients(zip(grads, [model.W, model.B]),\n",
    "                            global_step=tf.train.get_or_create_global_step())\n",
    "  if i % 20 == 0:\n",
    "    print(\"Loss at step {:03d}: {:.3f}\".format(i, loss(model, training_inputs, training_outputs)))\n",
    "\n",
    "print(\"Final loss: {:.3f}\".format(loss(model, training_inputs, training_outputs)))\n",
    "print(\"W = {}, B = {}\".format(model.W.numpy(), model.B.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use objects for state during eager execution\n",
    "\n",
    "With graph execution, program state (such as the variables) is stored in global collections and their lifetime is managed by the tf.Session object. In contrast, during eager execution the lifetime of state objects is determined by the lifetime of their corresponding Python object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables are objects\n",
    "During eager execution, variables persist until the last reference to the object is removed, and is then deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = tfe.Variable(tf.random_normal([1000, 1000]))\n",
    "v = None  # v no longer takes up GPU memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object-based saving\n",
    "tfe.Checkpoint can save and restore tfe.Variables to and from checkpoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=2.0>\n"
     ]
    }
   ],
   "source": [
    "x = tfe.Variable(10.)\n",
    "\n",
    "checkpoint = tfe.Checkpoint(x=x)  # save as \"x\"\n",
    "\n",
    "x.assign(2.)   # Assign a new value to the variables and save.\n",
    "save_path = checkpoint.save('./ckpt/')\n",
    "\n",
    "x.assign(11.)  # Change the variable after saving.\n",
    "\n",
    "# Restore values from the checkpoint\n",
    "checkpoint.restore(save_path)\n",
    "\n",
    "print(x)  # => 2.0\n",
    "\n",
    "# Cool cool cool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save and load models, tfe.Checkpoint stores the internal state of objects, without requiring hidden variables. To record the state of a model, an optimizer, and a global step, pass them to a tfe.Checkpoint:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object-oriented metrics\n",
    "tfe.metrics are stored as objects. Update a metric by passing the new data to the callable, and retrieve the result using the tfe.metrics.result method, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=18110, shape=(), dtype=float64, numpy=5.5>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = tfe.metrics.Mean(\"loss\")\n",
    "m(0)\n",
    "m(5)\n",
    "m.result()  # => 2.5\n",
    "m([8, 9])\n",
    "m.result()  # => 5.5\n",
    "\n",
    "# So there are some metrics in which you can pass\n",
    "# values, then do m.result() and it will output the result\n",
    "# of the previous inputs... hmm wonder how many there are\n",
    "\n",
    "# Not as many as i'd expect... seems like because it's Eager.\n",
    "# i'm going to have to go lower level after I complete my logistic\n",
    "# classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summaries and TensorBoard\n",
    "TensorBoard is a visualization tool for understanding, debugging and optimizing the model training process. It uses summary events that are written while executing the program.\n",
    "\n",
    "tf.contrib.summary is compatible with both eager and graph execution environments. Summary operations, such as tf.contrib.summary.scalar, are inserted during model construction. For example, to record summaries once every 100 global steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writer = tf.contrib.summary.create_file_writer(logdir)\n",
    "# global_step=tf.train.get_or_create_global_step()  # return global step var\n",
    "\n",
    "# writer.set_as_default()\n",
    "\n",
    "# for _ in range(iterations):\n",
    "#   global_step.assign_add(1)\n",
    "#   # Must include a record_summaries method\n",
    "#   with tf.contrib.summary.record_summaries_every_n_global_steps(100):\n",
    "#     # your model code goes here\n",
    "#     tf.contrib.summary.scalar('loss', loss)\n",
    "#     # i'm not exactly sure how this works. but it seems like a way\n",
    "#     # to track how your model performs during training,\n",
    "    \n",
    "# # TensorBoard seems like its own whole thing as well. Apperantly it's\n",
    "# # useful for Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance\n",
    "Computation is automatically offloaded to GPUs during eager execution. If you want control over where a computation runs you can enclose it in a tf.device('/gpu:0') block (or the CPU equivalent):\n",
    "\n",
    "A tf.Tensor object can be copied to a different device to execute its operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# good to know, not relevant as of right now for me \n",
    "# but looks quite intuitive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work with graphs\n",
    "While eager execution makes development and debugging more interactive, TensorFlow graph execution has advantages for distributed training, performance optimizations, and production deployment. However, writing graph code can feel different than writing regular Python code and more difficult to debug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eager is more debug friendly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write compatible code\n",
    "The same code written for eager execution will also build a graph during graph execution. Do this by simply running the same code in a new Python session where eager execution is not enabled.\n",
    "\n",
    "1) Use tf.data for input processing instead of queues. It's faster and easier.\n",
    "\n",
    "2) Use object-oriented layer APIs—like tf.keras.layers and tf.keras.Model—since they have explicit storage for variables.\n",
    "\n",
    "3) Once eager execution is enabled with tf.enable_eager_execution, it cannot be turned off. Start a new Python session to return to graph execution.\n",
    "\n",
    "Write, debug, and iterate in eager execution, then import the model graph for production deployment. Use tfe.Checkpoint to save and restore model variables, this allows movement between eager and graph execution environments. See the examples in: tensorflow/contrib/eager/python/examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok, Eager for development, graph for the final version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Eager Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going to now copy their example for a linear classifier, and copy it, then implement a logistic classifier based on it by overloading functions "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eager Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow's eager execution evaluates operations immediately, without an extra graph-building step. Operations return concrete values instead of constructing a computational graph to run later. This makes it easy to get started with TensorFlow, debug models, reduce boilerplate code, and is ~~fun~~ amazing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup and basic usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello, [[4.]]\n"
     ]
    }
   ],
   "source": [
    "tf.executing_eagerly()\n",
    "\n",
    "x = [[2.]]\n",
    "m = tf.matmul(x, x) \n",
    "print(\"hello, {}\".format(m))\n",
    "# tf.Tensor objects reference concrete values instead of \n",
    "# symbolic handles to nodes in a computational graph.           COOL\n",
    "# Since there isn't a computational graph to build and run later \n",
    "# in a session, it's easy to inspect results using print() or a debugger.\n",
    "\n",
    "# TensorFlow math operations convert Python objects \n",
    "# and NumPy arrays to tf.Tensor objects. The tf.Tensor.numpy    THANK GOD\n",
    "# method returns the object's value as a NumPy ndarray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1 2]\n",
      " [3 4]], shape=(2, 2), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[2 3]\n",
      " [4 5]], shape=(2, 2), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[ 2  6]\n",
      " [12 20]], shape=(2, 2), dtype=int32)\n",
      "-------\n",
      "[[ 2  6]\n",
      " [12 20]]\n",
      "[[1 2]\n",
      " [3 4]]\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([[1, 2],\n",
    "                 [3, 4]])\n",
    "print(a)\n",
    "\n",
    "# Broadcasting support\n",
    "b = tf.add(a, 1) #a+1\n",
    "print(b)\n",
    "\n",
    "# Operator overloading is supported\n",
    "print(a * b)     #a.*b    NEAT!!\n",
    "print(\"-------\")\n",
    "\n",
    "# Use NumPy values\n",
    "import numpy as np\n",
    "c = np.multiply(a, b)\n",
    "print(c)\n",
    "\n",
    "# Obtain numpy value from a tensor:\n",
    "print(a.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eager training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: id=48, shape=(1, 1), dtype=float32, numpy=array([[2.]], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.contrib.eager as tfe\n",
    "\n",
    "w = tfe.Variable([[1.0]])\n",
    "with tfe.GradientTape() as tape:\n",
    "  loss = w * w\n",
    "\n",
    "grad = tape.gradient(loss, [w])\n",
    "print(grad) # getting the hang of it, you can print out details of any\n",
    "            # tf object\n",
    "    \n",
    "# During eager execution, use tfe.GradientTape to trace \n",
    "# operations for computing gradients later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 69.959\n",
      "Loss at step 000: 67.229\n",
      "Loss at step 020: 30.506\n",
      "Loss at step 040: 14.117\n",
      "Loss at step 060: 6.802\n",
      "Loss at step 080: 3.538\n",
      "Loss at step 100: 2.081\n",
      "Loss at step 120: 1.431\n",
      "Loss at step 140: 1.141\n",
      "Loss at step 160: 1.011\n",
      "Loss at step 180: 0.953\n",
      "Final loss: 0.929\n",
      "W = 3.0279805660247803, B = 2.0723013877868652\n"
     ]
    }
   ],
   "source": [
    "# A toy dataset of points around 3 * x + 2\n",
    "NUM_EXAMPLES = 1000\n",
    "training_inputs = tf.random_normal([NUM_EXAMPLES]) #generates 1000 #s 0-1\n",
    "noise = tf.random_normal([NUM_EXAMPLES])\n",
    "training_outputs = training_inputs * 3 + 2 + noise\n",
    "\n",
    "def prediction(input, weight, bias):\n",
    "  return input * weight + bias\n",
    "\n",
    "# A loss function using mean-squared error\n",
    "def loss(weights, biases):\n",
    "  error = prediction(training_inputs, weights, biases) - training_outputs\n",
    "  return tf.reduce_mean(tf.square(error)) \n",
    "    #handles matrices, very cool\n",
    "\n",
    "# Return the derivative of loss with respect to weight and bias\n",
    "def grad(weights, biases):\n",
    "  with tfe.GradientTape() as tape:\n",
    "    loss_value = loss(weights, biases) \n",
    "  return tape.gradient(loss_value, [weights, biases]) \n",
    "    # tape.gradient(squared_error, [theta, biases])\n",
    "    # returns derivatives of loss for each theta! neat. \n",
    "    # Note to self: seperate biases from weights\n",
    "\n",
    "train_steps = 200\n",
    "learning_rate = 0.01\n",
    "# Start with arbitrary values for W and B on the same batch of data\n",
    "W = tfe.Variable(5.) #theta\n",
    "B = tfe.Variable(10.) #theta(1) in octave\n",
    "#standard variable, in this case a decimal\n",
    "\n",
    "print(\"Initial loss: {:.3f}\".format(loss(W, B)))\n",
    "\n",
    "for i in range(train_steps):\n",
    "  dW, dB = grad(W, B)\n",
    "  W.assign_sub(dW * learning_rate) # W -=('gradient' * alpha)\n",
    "  B.assign_sub(dB * learning_rate)\n",
    "    #basically gradient descent for a simple linear classifier\n",
    "  if i % 20 == 0:\n",
    "    print(\"Loss at step {:03d}: {:.3f}\".format(i, loss(W, B)))\n",
    "\n",
    "print(\"Final loss: {:.3f}\".format(loss(W, B)))\n",
    "print(\"W = {}, B = {}\".format(W.numpy(), B.numpy()))\n",
    "\n",
    "# basically in this example: \n",
    "# tfe.GradientTape() as tape, \n",
    "# can do, tape.gradient(loss_value, [weights, biases]\n",
    "# which takes: cost_fcn, [theta(2:end), theta(1)]\n",
    "# returns: vector of derivative (-/+ int)\n",
    "\n",
    "# *Can also be used to do:\n",
    "# backpropagation,\n",
    "# derivatives in respect to things, \n",
    "# partial derivatives,\n",
    "# *And can be:\n",
    "# overloaded,\n",
    "\n",
    "# Beautiful. (& complicated!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and train models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eager execution encourages the use of the Keras-style layer classes in the \"tf.keras.layers\" module. Additionally, the \"tf.train.Optimizer\" classes provide sophisticated techniques to calculate parameter updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Dense(10, input_shape=(784,)),  # must declare input shape\n",
    "  tf.keras.layers.Dense(10)\n",
    "]) #tf includes Keras in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

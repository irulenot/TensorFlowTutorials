{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eager Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow's eager execution evaluates operations immediately, without an extra graph-building step. Operations return concrete values instead of constructing a computational graph to run later. This makes it easy to get started with TensorFlow, debug models, reduce boilerplate code, and is ~~fun~~ amazing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup and basic usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello, [[4.]]\n"
     ]
    }
   ],
   "source": [
    "tf.executing_eagerly()\n",
    "\n",
    "x = [[2.]]\n",
    "m = tf.matmul(x, x) \n",
    "print(\"hello, {}\".format(m))\n",
    "# tf.Tensor objects reference concrete values instead of \n",
    "# symbolic handles to nodes in a computational graph.           COOL\n",
    "# Since there isn't a computational graph to build and run later \n",
    "# in a session, it's easy to inspect results using print() or a debugger.\n",
    "\n",
    "# TensorFlow math operations convert Python objects \n",
    "# and NumPy arrays to tf.Tensor objects. The tf.Tensor.numpy    THANK GOD\n",
    "# method returns the object's value as a NumPy ndarray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1 2]\n",
      " [3 4]], shape=(2, 2), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[2 3]\n",
      " [4 5]], shape=(2, 2), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[ 2  6]\n",
      " [12 20]], shape=(2, 2), dtype=int32)\n",
      "-------\n",
      "[[ 2  6]\n",
      " [12 20]]\n",
      "[[1 2]\n",
      " [3 4]]\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([[1, 2],\n",
    "                 [3, 4]])\n",
    "print(a)\n",
    "\n",
    "# Broadcasting support\n",
    "b = tf.add(a, 1) #a+1\n",
    "print(b)\n",
    "\n",
    "# Operator overloading is supported\n",
    "print(a * b)     #a.*b    NEAT!!\n",
    "print(\"-------\")\n",
    "\n",
    "# Use NumPy values\n",
    "import numpy as np\n",
    "c = np.multiply(a, b)\n",
    "print(c)\n",
    "\n",
    "# Obtain numpy value from a tensor:\n",
    "print(a.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eager training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "[<tf.Tensor: id=31, shape=(1, 1), dtype=float32, numpy=array([[2.]], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.contrib.eager as tfe\n",
    "\n",
    "w = tfe.Variable([[1.0]])\n",
    "with tfe.GradientTape() as tape:\n",
    "  loss = w * w\n",
    "\n",
    "grad = tape.gradient(loss, [w])\n",
    "print(grad) # getting the hang of it, you can print out details of any\n",
    "            # tf object\n",
    "    \n",
    "# During eager execution, use tfe.GradientTape to trace \n",
    "# operations for computing gradients later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 69.396\n",
      "Loss at step 000: 66.681\n",
      "Loss at step 020: 30.206\n",
      "Loss at step 040: 13.978\n",
      "Loss at step 060: 6.758\n",
      "Loss at step 080: 3.545\n",
      "Loss at step 100: 2.115\n",
      "Loss at step 120: 1.479\n",
      "Loss at step 140: 1.196\n",
      "Loss at step 160: 1.070\n",
      "Loss at step 180: 1.014\n",
      "Final loss: 0.990\n",
      "W = 3.0184741020202637, B = 2.1261708736419678\n"
     ]
    }
   ],
   "source": [
    "# A toy dataset of points around 3 * x + 2\n",
    "NUM_EXAMPLES = 1000\n",
    "training_inputs = tf.random_normal([NUM_EXAMPLES]) #generates 1000 #s 0-1\n",
    "noise = tf.random_normal([NUM_EXAMPLES])\n",
    "training_outputs = training_inputs * 3 + 2 + noise\n",
    "\n",
    "def prediction(input, weight, bias):\n",
    "  return input * weight + bias\n",
    "\n",
    "# A loss function using mean-squared error\n",
    "def loss(weights, biases):\n",
    "  error = prediction(training_inputs, weights, biases) - training_outputs\n",
    "  return tf.reduce_mean(tf.square(error)) \n",
    "    #handles matrices, very cool\n",
    "\n",
    "# Return the derivative of loss with respect to weight and bias\n",
    "def grad(weights, biases):\n",
    "  with tfe.GradientTape() as tape:\n",
    "    loss_value = loss(weights, biases) \n",
    "  return tape.gradient(loss_value, [weights, biases]) \n",
    "    # tape.gradient(squared_error, [theta, biases])\n",
    "    # returns derivatives of loss for each theta! neat. \n",
    "    # Note to self: seperate biases from weights\n",
    "\n",
    "train_steps = 200\n",
    "learning_rate = 0.01\n",
    "# Start with arbitrary values for W and B on the same batch of data\n",
    "W = tfe.Variable(5.) #theta\n",
    "B = tfe.Variable(10.) #theta(1) in octave\n",
    "#standard variable, in this case a decimal\n",
    "\n",
    "print(\"Initial loss: {:.3f}\".format(loss(W, B)))\n",
    "\n",
    "for i in range(train_steps):\n",
    "  dW, dB = grad(W, B)\n",
    "  W.assign_sub(dW * learning_rate) # W -=('gradient' * alpha)\n",
    "  B.assign_sub(dB * learning_rate)\n",
    "    #basically gradient descent for a simple linear classifier\n",
    "  if i % 20 == 0:\n",
    "    print(\"Loss at step {:03d}: {:.3f}\".format(i, loss(W, B)))\n",
    "\n",
    "print(\"Final loss: {:.3f}\".format(loss(W, B)))\n",
    "print(\"W = {}, B = {}\".format(W.numpy(), B.numpy()))\n",
    "\n",
    "# basically in this example: \n",
    "# tfe.GradientTape() as tape, \n",
    "# can do, tape.gradient(loss_value, [weights, biases]\n",
    "# which takes: cost_fcn, [theta(2:end), theta(1)]\n",
    "# returns: vector of derivative (-/+ int)\n",
    "\n",
    "# *Can also be used to do:\n",
    "# backpropagation,\n",
    "# derivatives in respect to things, \n",
    "# partial derivatives,\n",
    "# *And can be:\n",
    "# overloaded,\n",
    "\n",
    "# Beautiful. (& complicated!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and train models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eager execution encourages the use of the Keras-style layer classes in the \"tf.keras.layers\" module. Additionally, the \"tf.train.Optimizer\" classes provide sophisticated techniques to calculate parameter updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Dense(10, input_shape=(784,)),  # must declare input shape\n",
    "  tf.keras.layers.Dense(10)\n",
    "]) \n",
    "#tf includes Keras in it! Can derive own class from Keras as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Andd of course they updated the tutorial while I was in the middle of doing it. That confused me for a good 20 min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a model\n",
    "While you can use any Python object to represent a layer, TensorFlow has tf.keras.layers.Layer as a convenient base class. Inherit from it to implement your own layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Dense(10, input_shape=(784,)),  # must declare input shape\n",
    "  tf.keras.layers.Dense(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 784)\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor representing a blank image\n",
    "batch = tf.zeros([1, 1, 784])\n",
    "print(batch.shape)  # => (1, 1, 784)\n",
    "\n",
    "result = model(batch)\n",
    "# => tf.Tensor([[[ 0.  0., ..., 0.]]], shape=(1, 1, 10), dtype=float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset  # download dataset.py file\n",
    "dataset_train = dataset.train('./datasets').shuffle(60000).repeat(4).batch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a model, define a loss function to optimize and then calculate gradients. Use an optimizer to update the variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'BatchDataset' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-ad286a763c5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Gradient optimizer is apperantly an object we use\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Initial loss: {:.3f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'BatchDataset' object is not iterable"
     ]
    }
   ],
   "source": [
    "def loss(model, x, y):\n",
    "  prediction = model(x)\n",
    "  return tf.losses.sparse_softmax_cross_entropy(labels=y, logits=prediction)\n",
    "    # loss function, tf.losses is lib with loss functions\n",
    "    # sparse_softmax_cross_entropy is evidently one of their functions\n",
    "\n",
    "def grad(model, inputs, targets):\n",
    "  with tf.GradientTape() as tape:\n",
    "    loss_value = loss(model, inputs, targets)\n",
    "  return tape.gradient(loss_value, model.variables)\n",
    "    # keras object has .variables, which nicely passes into tape.gradient\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "# Gradient optimizer is apperantly an object we use\n",
    "\n",
    "x, y = iter(dataset_train).next()\n",
    "print(\"Initial loss: {:.3f}\".format(loss(model, x, y)))\n",
    "\n",
    "# Training loop\n",
    "for (i, (x, y)) in enumerate(dataset_train):\n",
    "  # Calculate derivatives of the input function with respect to its parameters.\n",
    "  grads = grad(model, x, y)\n",
    "  # Apply the gradient to the model\n",
    "  optimizer.apply_gradients(zip(grads, model.variables),\n",
    "                            global_step=tf.train.get_or_create_global_step())\n",
    "    # kinda gross, but the optimizer object gets the above inputs (model info)\n",
    "    # and a global step tensor to do gradient descent\n",
    "    # still better than implementing backpropogation myself! Hah (even if it might be fun...)\n",
    "    \n",
    "    \n",
    "  if i % 200 == 0:\n",
    "    print(\"Loss at step {:04d}: {:.3f}\".format(i, loss(model, x, y)))\n",
    "\n",
    "print(\"Final loss: {:.3f}\".format(loss(model, x, y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables and optimizers\n",
    "tfe.Variable objects store mutable tf.Tensor values accessed during training to make automatic differentiation easier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use objects for state during eager execution\n",
    "\n",
    "With graph execution, program state (such as the variables) is stored in global collections and their lifetime is managed by the tf.Session object. In contrast, during eager execution the lifetime of state objects is determined by the lifetime of their corresponding Python object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

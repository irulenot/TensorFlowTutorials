{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective\n",
    "\n",
    "I'm going to create a linear predictor in TF based on the one provided. I will first get it working in this Jupyter document, mark it up with comments then make my own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.contrib.eager.python.tfe' has no attribute 'random_normal'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-ecbed1e92d84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mNUM_EXAMPLES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtraining_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mNUM_EXAMPLES\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mNUM_EXAMPLES\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtraining_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_inputs\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow.contrib.eager.python.tfe' has no attribute 'random_normal'"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "NUM_EXAMPLES = 1000\n",
    "training_inputs = tf.random_normal([NUM_EXAMPLES])\n",
    "noise = tf.random_normal([NUM_EXAMPLES])\n",
    "training_outputs = training_inputs * 3 + 2 + noise\n",
    "\n",
    "# from tensorflow.python.keras.layers import Input, Dense\n",
    "# a = Input(shape=(1,))\n",
    "# b = Dense(1)(a)\n",
    "# model = Model(inputs=a, outputs=b)\n",
    "\n",
    "import linear_regression as tflr\n",
    "linear_model = tflr.LinearModel()\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((training_inputs, training_outputs))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "# tflr.fit(linear_model, dataset, optimizer, verbose=True, logdir=None) #Throws runtime error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, well my plan to get it working didn't work. Guess i'll make it on my own!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier:\n",
    "    \n",
    "    weights = tfe.Variable(0)\n",
    "    bias = tfe.Variable(1)\n",
    "    learning_rate = 0.01\n",
    "    train_steps = 200\n",
    "    \n",
    "    def define(self, weights, bias, train_steps):\n",
    "        self.theta = weights\n",
    "        self.alpha = bias\n",
    "        self.epoch = train_steps\n",
    "    \n",
    "        def predict(self, Xhat):\n",
    "              return Xhat * self.weights + self.bias\n",
    "    \n",
    "#     # Given training data Xtr and Ytr\n",
    "#     def train(self, Xtr, Ytr):\n",
    "#         Ytr = np.matrix(Ytr)\n",
    "#         Xtr = np.matrix(Xtr)\n",
    "#         Ytr = Ytr.T\n",
    "#         self.gradient_des(Xtr, Ytr)\n",
    "        \n",
    "#     # Given predictions(Yhat) and correct labels(Yte)\n",
    "#     # Returns squared error\n",
    "#     def cost_fcn(self, Yhat, Yte):\n",
    "#         squared_error = (Yhat - Yte)**2\n",
    "#         b = 1/(Yhat.size*2)\n",
    "#         return b * np.sum(squared_error, 0)\n",
    "    \n",
    "#     # Given training data Xtr and Ytr\n",
    "#     # Graphs error over iterations of gradient descent\n",
    "#     def gradient_des(self, Xtr, Ytr):\n",
    "#         j2 = np.zeros(self.epoch)\n",
    "#         i2 = np.zeros(self.epoch)\n",
    "#         Xtr = np.insert(Xtr, 0, 1, axis=1)\n",
    "        \n",
    "#         for i in range(0, self.epoch):\n",
    "#             Yhat = self.predict(Xtr)\n",
    "#             size = Xtr.shape[1]\n",
    "#             self.theta -= self.alpha * (1/size) * np.matmul((Yhat - Ytr).T, Xtr)\n",
    "#             j2[i] = self.cost_fcn(Yhat.A1, Ytr.A1)  # .A1 turns matrix -> array\n",
    "#             i2[i] = i\n",
    "#         plt.xlabel(\"Iterations\")\n",
    "#         plt.ylabel(\"Error\")\n",
    "#         plt.plot(i2, j2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported format string passed to Tensor.__format__",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-c3f96e191daa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m#standard variable, in this case a decimal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Initial loss: {:.3f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported format string passed to Tensor.__format__"
     ]
    }
   ],
   "source": [
    "# A toy dataset of points around 3 * x + 2\n",
    "NUM_EXAMPLES = 1000\n",
    "training_inputs = tf.random_normal([NUM_EXAMPLES]) #generates 1000 #s 0-1\n",
    "noise = tf.random_normal([NUM_EXAMPLES])\n",
    "training_outputs = training_inputs * 3 + 2 + noise\n",
    "\n",
    "model = LinearClassifier()\n",
    "model.predict(training_inputs)\n",
    "\n",
    "# # A loss function using mean-squared error\n",
    "# def loss(weights, biases):\n",
    "#   error = prediction(training_inputs, weights, biases) - training_outputs\n",
    "#   return tf.reduce_mean(tf.square(error)) \n",
    "#     #handles matrices, very cool\n",
    "\n",
    "# # Return the derivative of loss with respect to weight and bias\n",
    "# def grad(weights, biases):\n",
    "#   with tfe.GradientTape() as tape:\n",
    "#     loss_value = loss(weights, biases) \n",
    "#   return tape.gradient(loss_value, [weights, biases]) \n",
    "#     # tape.gradient(squared_error, [theta, biases])\n",
    "#     # returns derivatives of loss for each theta! neat. \n",
    "#     # Note to self: seperate biases from weights\n",
    "\n",
    "# train_steps = 200\n",
    "# learning_rate = 0.01\n",
    "# # Start with arbitrary values for W and B on the same batch of data\n",
    "# W = tfe.Variable(5.) #theta\n",
    "# B = tfe.Variable(10.) #theta(1) in octave\n",
    "# #standard variable, in this case a decimal\n",
    "\n",
    "# print(\"Initial loss: {:.3f}\".format(loss(W, B)))\n",
    "\n",
    "# for i in range(train_steps):\n",
    "#   dW, dB = grad(W, B)\n",
    "#   W.assign_sub(dW * learning_rate) # W -=('gradient' * alpha)\n",
    "#   B.assign_sub(dB * learning_rate)\n",
    "#     #basically gradient descent for a simple linear classifier\n",
    "#   if i % 20 == 0:\n",
    "#     print(\"Loss at step {:03d}: {:.3f}\".format(i, loss(W, B)))\n",
    "\n",
    "# print(\"Final loss: {:.3f}\".format(loss(W, B)))\n",
    "# print(\"W = {}, B = {}\".format(W.numpy(), B.numpy()))\n",
    "\n",
    "# # basically in this example: \n",
    "# # tfe.GradientTape() as tape, \n",
    "# # can do, tape.gradient(loss_value, [weights, biases]\n",
    "# # which takes: cost_fcn, [theta(2:end), theta(1)]\n",
    "# # returns: vector of derivative (-/+ int)\n",
    "\n",
    "# # *Can also be used to do:\n",
    "# # backpropagation,\n",
    "# # derivatives in respect to things, \n",
    "# # partial derivatives,\n",
    "# # *And can be:\n",
    "# # overloaded,\n",
    "\n",
    "# # Beautiful. (& complicated!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
